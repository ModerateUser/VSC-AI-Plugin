# üöÄ GPU Support Guide

Complete guide to enabling GPU acceleration for the Agentic Workflow Plugin.

---

## üìã Table of Contents

1. [Why GPU Support?](#why-gpu-support)
2. [GPU Requirements](#gpu-requirements)
3. [Installation by Platform](#installation-by-platform)
4. [Configuration](#configuration)
5. [Verification](#verification)
6. [Performance Comparison](#performance-comparison)
7. [Troubleshooting](#troubleshooting)

---

## üéØ Why GPU Support?

GPU acceleration provides **10-100x faster** AI model inference compared to CPU:

| Operation | CPU Time | GPU Time | Speedup |
|-----------|----------|----------|---------|
| Text Generation (1000 tokens) | 30s | 2s | **15x** |
| Embedding Generation (100 docs) | 45s | 3s | **15x** |
| Vector Search (10k vectors) | 5s | 0.2s | **25x** |
| Model Loading | 10s | 2s | **5x** |

**GPU is essential for:**
- Large language models (7B+ parameters)
- Real-time inference
- Batch processing
- Production workloads

---

## üíª GPU Requirements

### NVIDIA GPUs (Recommended)

| Component | Minimum | Recommended | Optimal |
|-----------|---------|-------------|---------|
| **GPU** | GTX 1060 (6GB) | RTX 3060 (12GB) | RTX 4090 (24GB) |
| **VRAM** | 6GB | 12GB | 24GB+ |
| **CUDA** | 11.8+ | 12.0+ | 12.1+ |
| **Driver** | 520+ | 535+ | Latest |

### AMD GPUs (Experimental)

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **GPU** | RX 6600 (8GB) | RX 7900 XTX (24GB) |
| **VRAM** | 8GB | 16GB+ |
| **ROCm** | 5.4+ | 5.7+ |

### Apple Silicon (M1/M2/M3)

| Component | Support | Notes |
|-----------|---------|-------|
| **M1/M2/M3** | ‚úÖ Full | Uses Metal Performance Shaders |
| **Unified Memory** | 8GB+ | 16GB+ recommended |

---

## üîß Installation by Platform

### Windows (NVIDIA)

#### Step 1: Install NVIDIA Drivers

1. Check your GPU:
   ```cmd
   nvidia-smi
   ```

2. If not installed, download from [NVIDIA Drivers](https://www.nvidia.com/Download/index.aspx)

3. Verify installation:
   ```cmd
   nvidia-smi
   ```
   Should show GPU info and CUDA version

#### Step 2: Install CUDA Toolkit

```cmd
# Download CUDA 12.1 from NVIDIA
# https://developer.nvidia.com/cuda-downloads

# Verify installation
nvcc --version
```

#### Step 3: Install cuDNN

1. Download cuDNN from [NVIDIA cuDNN](https://developer.nvidia.com/cudnn)
2. Extract and copy files to CUDA directory
3. Add to PATH:
   ```cmd
   set PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin;%PATH%
   ```

#### Step 4: Install Python GPU Packages

```cmd
# Uninstall CPU versions
pip uninstall faiss-cpu torch

# Install GPU versions
pip install faiss-gpu
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install sentence-transformers transformers accelerate
```

---

### Linux (NVIDIA)

#### Step 1: Install NVIDIA Drivers

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install nvidia-driver-535

# Verify
nvidia-smi
```

#### Step 2: Install CUDA Toolkit

```bash
# Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install cuda-toolkit-12-1

# Add to PATH
echo 'export PATH=/usr/local/cuda-12.1/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# Verify
nvcc --version
```

#### Step 3: Install Python GPU Packages

```bash
# Uninstall CPU versions
pip uninstall faiss-cpu torch

# Install GPU versions
pip install faiss-gpu
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install sentence-transformers transformers accelerate
```

---

### macOS (Apple Silicon)

#### Step 1: Verify Metal Support

```bash
# Check system info
system_profiler SPDisplaysDataType | grep Metal
```

#### Step 2: Install Python Packages with Metal Support

```bash
# Install PyTorch with Metal support
pip install torch torchvision torchaudio

# Install other packages
pip install sentence-transformers transformers accelerate

# FAISS with Metal (if available)
pip install faiss-cpu  # Will use Metal acceleration on M1/M2/M3
```

#### Step 3: Enable Metal Performance Shaders

```bash
# Set environment variable
echo 'export PYTORCH_ENABLE_MPS_FALLBACK=1' >> ~/.zshrc
source ~/.zshrc
```

---

### Linux (AMD ROCm)

#### Step 1: Install ROCm

```bash
# Ubuntu 22.04
wget https://repo.radeon.com/amdgpu-install/latest/ubuntu/jammy/amdgpu-install_5.7.50700-1_all.deb
sudo apt install ./amdgpu-install_5.7.50700-1_all.deb
sudo amdgpu-install --usecase=rocm

# Verify
rocm-smi
```

#### Step 2: Install Python Packages

```bash
# Install PyTorch with ROCm
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

# Install other packages
pip install sentence-transformers transformers accelerate
```

---

## ‚öôÔ∏è Configuration

### Update VS Code Settings

Add GPU configuration to your settings:

```json
{
  // Enable GPU acceleration
  "agenticWorkflow.enableGPU": true,
  
  // GPU device selection (0 = first GPU, -1 = CPU)
  "agenticWorkflow.gpuDevice": 0,
  
  // GPU memory limit (in GB, 0 = no limit)
  "agenticWorkflow.gpuMemoryLimit": 0,
  
  // Batch size for GPU inference
  "agenticWorkflow.gpuBatchSize": 32,
  
  // Mixed precision (faster, uses less memory)
  "agenticWorkflow.enableMixedPrecision": true,
  
  // GPU memory fraction (0.0-1.0)
  "agenticWorkflow.gpuMemoryFraction": 0.9
}
```

### Configuration Options Explained

#### `enableGPU`
- **Type:** Boolean
- **Default:** `false`
- **Purpose:** Enable GPU acceleration
- **Note:** Requires GPU packages installed

#### `gpuDevice`
- **Type:** Number
- **Default:** `0`
- **Purpose:** Select GPU device (for multi-GPU systems)
- **Values:** 
  - `0` = First GPU
  - `1` = Second GPU
  - `-1` = Force CPU

#### `gpuMemoryLimit`
- **Type:** Number (GB)
- **Default:** `0` (no limit)
- **Purpose:** Limit GPU memory usage
- **Example:** `8` = Use max 8GB VRAM

#### `gpuBatchSize`
- **Type:** Number
- **Default:** `32`
- **Purpose:** Batch size for inference
- **Note:** Larger = faster but more memory

#### `enableMixedPrecision`
- **Type:** Boolean
- **Default:** `true`
- **Purpose:** Use FP16 instead of FP32
- **Benefit:** 2x faster, 50% less memory

#### `gpuMemoryFraction`
- **Type:** Number (0.0-1.0)
- **Default:** `0.9`
- **Purpose:** Reserve GPU memory percentage
- **Example:** `0.8` = Use 80% of VRAM

---

## ‚úÖ Verification

### Check GPU Detection

Create a test workflow: `test-gpu.workflow.json`

```json
{
  "name": "GPU Test",
  "version": "1.0.0",
  "nodes": [
    {
      "id": "test",
      "type": "script",
      "name": "Check GPU",
      "config": {
        "language": "python",
        "code": "import torch\nprint(f'CUDA Available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
      }
    }
  ]
}
```

Run the workflow and check output:
```
CUDA Available: True
GPU: NVIDIA GeForce RTX 4090
VRAM: 24.00 GB
```

### Benchmark GPU Performance

Create `benchmark-gpu.workflow.json`:

```json
{
  "name": "GPU Benchmark",
  "version": "1.0.0",
  "nodes": [
    {
      "id": "benchmark",
      "type": "script",
      "name": "Benchmark",
      "config": {
        "language": "python",
        "code": "import torch\nimport time\n\n# Test tensor operations\nsize = 10000\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nstart = time.time()\na = torch.randn(size, size, device=device)\nb = torch.randn(size, size, device=device)\nc = torch.matmul(a, b)\ntorch.cuda.synchronize() if device == 'cuda' else None\nend = time.time()\n\nprint(f'Device: {device}')\nprint(f'Matrix multiplication ({size}x{size}): {end-start:.4f}s')"
      }
    }
  ]
}
```

Expected results:
- **CPU:** ~5-10 seconds
- **GPU:** ~0.1-0.5 seconds

---

## üìä Performance Comparison

### Real-World Benchmarks

#### Text Generation (GPT-2 Medium)

| Hardware | Tokens/Second | Time for 1000 tokens |
|----------|---------------|---------------------|
| CPU (i9-13900K) | 15 | 67s |
| GPU (RTX 3060) | 180 | 5.6s |
| GPU (RTX 4090) | 450 | 2.2s |
| Apple M2 Max | 120 | 8.3s |

#### Embedding Generation (sentence-transformers)

| Hardware | Documents/Second | Time for 1000 docs |
|----------|------------------|-------------------|
| CPU (i9-13900K) | 25 | 40s |
| GPU (RTX 3060) | 350 | 2.9s |
| GPU (RTX 4090) | 800 | 1.3s |
| Apple M2 Max | 200 | 5s |

#### Vector Search (FAISS)

| Hardware | Queries/Second | Time for 10k queries |
|----------|----------------|---------------------|
| CPU (i9-13900K) | 2000 | 5s |
| GPU (RTX 3060) | 50000 | 0.2s |
| GPU (RTX 4090) | 100000 | 0.1s |
| Apple M2 Max | 30000 | 0.33s |

---

## üêõ Troubleshooting

### Issue: "CUDA not available"

**Check CUDA installation:**
```bash
# Windows
nvcc --version
nvidia-smi

# Linux
nvcc --version
nvidia-smi

# Check PyTorch CUDA
python -c "import torch; print(torch.cuda.is_available())"
```

**Solutions:**
1. Reinstall NVIDIA drivers
2. Reinstall CUDA toolkit
3. Reinstall PyTorch with CUDA:
   ```bash
   pip uninstall torch
   pip install torch --index-url https://download.pytorch.org/whl/cu121
   ```

### Issue: "Out of memory"

**Solutions:**

1. **Reduce batch size:**
   ```json
   {
     "agenticWorkflow.gpuBatchSize": 16
   }
   ```

2. **Set memory limit:**
   ```json
   {
     "agenticWorkflow.gpuMemoryLimit": 8
   }
   ```

3. **Enable mixed precision:**
   ```json
   {
     "agenticWorkflow.enableMixedPrecision": true
   }
   ```

4. **Use smaller models:**
   - Instead of `bert-large`: use `bert-base`
   - Instead of `gpt2-large`: use `gpt2-medium`

### Issue: "Slow GPU performance"

**Check GPU utilization:**
```bash
# Monitor GPU usage
nvidia-smi -l 1
```

**Solutions:**

1. **Increase batch size:**
   ```json
   {
     "agenticWorkflow.gpuBatchSize": 64
   }
   ```

2. **Check for CPU bottleneck:**
   - Ensure data loading is fast
   - Use SSD for model cache
   - Increase `maxConcurrentNodes`

3. **Update drivers:**
   ```bash
   # Check for driver updates
   nvidia-smi
   ```

### Issue: "Multiple GPUs not detected"

**List all GPUs:**
```python
import torch
print(f"GPU count: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
```

**Select specific GPU:**
```json
{
  "agenticWorkflow.gpuDevice": 1
}
```

### Issue: "Apple Silicon not using GPU"

**Check Metal support:**
```python
import torch
print(f"MPS available: {torch.backends.mps.is_available()}")
print(f"MPS built: {torch.backends.mps.is_built()}")
```

**Enable MPS:**
```bash
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

---

## üéØ Best Practices

### 1. Memory Management

```json
{
  // Reserve 10% for system
  "agenticWorkflow.gpuMemoryFraction": 0.9,
  
  // Set hard limit for safety
  "agenticWorkflow.gpuMemoryLimit": 20,
  
  // Use mixed precision
  "agenticWorkflow.enableMixedPrecision": true
}
```

### 2. Batch Processing

```json
{
  // Larger batches = better GPU utilization
  "agenticWorkflow.gpuBatchSize": 64,
  
  // But don't exceed memory
  "agenticWorkflow.gpuMemoryLimit": 16
}
```

### 3. Multi-GPU Setup

```json
{
  // Use first GPU for inference
  "agenticWorkflow.gpuDevice": 0,
  
  // Use second GPU for embeddings (future feature)
  "agenticWorkflow.embeddingGpuDevice": 1
}
```

### 4. Model Selection

Choose GPU-optimized models:
- ‚úÖ `distilbert-base-uncased` (66M params)
- ‚úÖ `bert-base-uncased` (110M params)
- ‚úÖ `gpt2` (124M params)
- ‚ö†Ô∏è `bert-large-uncased` (340M params) - Needs 8GB+ VRAM
- ‚ö†Ô∏è `gpt2-xl` (1.5B params) - Needs 16GB+ VRAM

---

## üìà Optimization Tips

### 1. Model Quantization

Reduce model size and increase speed:

```python
# In your workflow script node
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    torch_dtype=torch.float16,  # Use FP16
    device_map="auto"
)
```

### 2. Flash Attention

For transformer models:

```bash
pip install flash-attn
```

### 3. Compile Models (PyTorch 2.0+)

```python
import torch

model = torch.compile(model)  # 2x faster inference
```

### 4. Persistent GPU Memory

Keep models loaded between runs:

```json
{
  "agenticWorkflow.keepModelsLoaded": true,
  "agenticWorkflow.modelCacheSize": 3
}
```

---

## üöÄ Next Steps

1. **Install GPU packages** for your platform
2. **Configure settings** in VS Code
3. **Run verification** workflow
4. **Benchmark performance** with your models
5. **Optimize settings** based on your GPU

---

## üìö Additional Resources

- **NVIDIA CUDA:** https://developer.nvidia.com/cuda-toolkit
- **PyTorch GPU:** https://pytorch.org/get-started/locally/
- **FAISS GPU:** https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU
- **Hugging Face Accelerate:** https://huggingface.co/docs/accelerate/

---

**Last Updated:** November 18, 2025  
**Version:** 0.1.0  
**Maintainer:** ModerateUser
